{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIS680_Fall2019_RLFinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHbNxdP6eLd2",
        "colab_type": "text"
      },
      "source": [
        "# Google Drive\n",
        "\n",
        "This first code block attaches your google drive and makes a folder structure. You only need to run this when a new VM is assigned to you. To get your code as a single python file go through the following menus File->'Download .py'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYICcmTfdm0J",
        "colab_type": "code",
        "outputId": "c26e1bfb-c494-4164-f92f-7ee114d5e587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount google drive\n",
        "DRIVE_MOUNT='/content/gdrive'\n",
        "drive.mount(DRIVE_MOUNT)\n",
        "\n",
        "# create folder to write data to\n",
        "CIS680_FOLDER=os.path.join(DRIVE_MOUNT, 'My Drive', 'CIS680_2019')\n",
        "HOMEWORK_FOLDER=os.path.join(CIS680_FOLDER, 'FinalProject')\n",
        "os.makedirs(HOMEWORK_FOLDER, exist_ok=True)\n",
        "checkpoint_path = os.path.join(HOMEWORK_FOLDER, \"params\")\n",
        "os.makedirs(checkpoint_path, exist_ok=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J9W9xngyE0w",
        "colab_type": "text"
      },
      "source": [
        "# Software installation\n",
        "\n",
        "The OpenAI Gym library we are will use for the RL simulation environment is installed here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMOhiMyYyBeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym[all] pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install box2d-py gym[box2d] > /dev/null 2>&1\n",
        "!pip install visdom > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBM93W4w0NEz",
        "colab_type": "text"
      },
      "source": [
        "# Start a virtual display\n",
        "\n",
        "A virtual display is needed to communicate with the X server in the backend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rko_QaByPRA",
        "colab_type": "code",
        "outputId": "ca39e0c4-ff00-4ca8-d080-2ed4e56d7391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw-yUtFqiNFR",
        "colab_type": "text"
      },
      "source": [
        "# Import Car Racing Environment\n",
        "\n",
        "Use reinforcement learning to race a car around a track.\n",
        "\n",
        "Official environment documentation is provided at:\n",
        "\n",
        "https://gym.openai.com/envs/CarRacing-v0/\n",
        "\n",
        "From OpenAI:\n",
        "\n",
        "State consists of 96x96 pixels. Reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points. Episode finishes when all tiles are visited.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hguIwry8iH9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make('CarRacing-v0')\n",
        "# Uncomment below line for environment documentation\n",
        "# help(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIt1jvQH_IXQ",
        "colab_type": "code",
        "outputId": "755d2f5b-1aa8-4976-e66a-362a56d7feca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env.reset()\n",
        "frame = env.render(mode='rgb_array')\n",
        "\n",
        "plt.imshow(frame)\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Track generation: 1101..1386 -> 285-tiles track\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvNh3B3FiAPY",
        "colab_type": "text"
      },
      "source": [
        "# Implement Your Solution\n",
        "\n",
        "For training any models, it will be helpful to save checkpoints and reload from the most recent. This is due to time constraints inside of colab.\n",
        "\n",
        "While most of the research using these platforms implements model-free reinforcement learning algorithms, you may implement other approaches including model-based methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc6lMEV9Rtqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Beta\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "import visdom\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-40mzlMicZB",
        "colab_type": "text"
      },
      "source": [
        "# Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOwnCZ2EiZnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.gamma = 0.99\n",
        "        self.action_repeat = 8\n",
        "        self.img_stack = 4\n",
        "        self.seed = 0\n",
        "        self.render = False\n",
        "        self.vis = False\n",
        "        self.log_interval = 10\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "if use_cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "dtype_vec = np.dtype([('s', np.float64, (args.img_stack, 96, 96)), ('a', np.float64, (3,)), ('a_logp', np.float64),\n",
        "                       ('r', np.float64), ('s_', np.float64, (args.img_stack, 96, 96))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtRtz8IYi77u",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXz0m36gi64g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.cnn_layer1 = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.cnn_layer2 = nn.Sequential(\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2,),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.cnn_layer3 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.cnn_layer4 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.cnn_layer5 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.cnn_layer6 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.v_head = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.alpha_head = nn.Sequential(\n",
        "            nn.Linear(128, 3),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "        self.beta_head = nn.Sequential(\n",
        "            nn.Linear(128, 3),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "        self.apply(self._weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layer1(x)\n",
        "        x = self.cnn_layer2(x)\n",
        "        x = self.cnn_layer3(x)\n",
        "        x = self.cnn_layer4(x)\n",
        "        x = self.cnn_layer5(x)\n",
        "        x = self.cnn_layer6(x)\n",
        "        x = x.view(-1, 256)\n",
        "        v = self.v_head(x)\n",
        "        alpha = self.alpha_head(self.fc_layer(x)) + 1\n",
        "        beta = self.beta_head(self.fc_layer(x)) + 1\n",
        "        return (alpha, beta), v\n",
        "\n",
        "    @staticmethod\n",
        "    def _weights_init(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
        "            nn.init.constant_(m.bias, 0.1)\n",
        "\n",
        "class ACNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Actor-Critic Network for PPO\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ACNet, self).__init__()\n",
        "        self.cnn_base = nn.Sequential(  # input shape (4, 96, 96)\n",
        "            nn.Conv2d(args.img_stack, 8, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),  # activation\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2),  # (8, 47, 47)\n",
        "            nn.ReLU(),  # activation\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2),  # (16, 23, 23)\n",
        "            nn.ReLU(),  # activation\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2),  # (32, 11, 11)\n",
        "            nn.ReLU(),  # activation\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),  # (64, 5, 5)\n",
        "            nn.ReLU(),  # activation\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1),  # (128, 3, 3)\n",
        "            nn.ReLU(),  # activation\n",
        "        )  # output shape (256, 1, 1)\n",
        "        self.v = nn.Sequential(nn.Linear(256, 100), nn.ReLU(), nn.Linear(100, 1))\n",
        "        self.fc = nn.Sequential(nn.Linear(256, 100), nn.ReLU())\n",
        "        self.alpha_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
        "        self.beta_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
        "        self.apply(self._weights_init)\n",
        "\n",
        "    @staticmethod\n",
        "    def _weights_init(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
        "            nn.init.constant_(m.bias, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_base(x)\n",
        "        x = x.view(-1, 256)\n",
        "        v = self.v(x)\n",
        "        x = self.fc(x)\n",
        "        alpha = self.alpha_head(x) + 1\n",
        "        beta = self.beta_head(x) + 1\n",
        "\n",
        "        return (alpha, beta), v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-PnDgO9jJ82",
        "colab_type": "text"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGJcjYW8jGFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AgentTrain():\n",
        "    def __init__(self, max_norm, epsilon, epochs, num_buffer):\n",
        "        self.max_norm = max_norm\n",
        "        self.epsilon = epsilon\n",
        "        self.epochs = epochs\n",
        "        self.num_buffer = num_buffer\n",
        "        self.training_step = 0\n",
        "        self.acnet = ACNet().type(torch.float64).to(device)\n",
        "        self.buffer = np.empty(self.num_buffer, dtype=dtype_vec)\n",
        "        self.counter = 0\n",
        "        self.optimizer = optim.Adam(self.acnet.parameters(), lr=2e-4)\n",
        "\n",
        "    def policy(self, state):\n",
        "        state = torch.from_numpy(state).type(torch.float64).to(device).unsqueeze(0)\n",
        "        alpha, beta = self.acnet(state)[0]\n",
        "        distance = Beta(alpha, beta)\n",
        "        action = distance.sample().squeeze(0)\n",
        "        log_prob_a = distance.log_prob(action).sum(1)\n",
        "        action = action.cpu().numpy()\n",
        "        return action, log_prob_a.item()\n",
        "\n",
        "    def save_state(self, dtype_vec):\n",
        "        self.buffer[self.counter] = dtype_vec\n",
        "        self.counter += 1\n",
        "        flag = False\n",
        "        if self.counter >= self.num_buffer:\n",
        "            self.counter = 0\n",
        "            flag = True\n",
        "            return flag\n",
        "\n",
        "    def update(self):\n",
        "        self.training_step += 1\n",
        "        s = torch.tensor(self.buffer['s'], device=device).type(torch.float64)\n",
        "        a = torch.tensor(self.buffer['a'], device=device).type(torch.float64)\n",
        "        r = torch.tensor(self.buffer['r'], device=device).type(torch.float64).view(-1, 1)\n",
        "        s_ = torch.tensor(self.buffer['s_'], device=device).type(torch.float64)\n",
        "        log_prob_a_old = torch.tensor(self.buffer['log_prob_a'], dtype=torch.double).to(device).view(-1, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            v_output = r + 0.99 * self.acnet(s_)[1]\n",
        "            ad_v = v_output - self.acnet(s)[1]\n",
        "\n",
        "        for k in range(self.epochs):\n",
        "            for index in BatchSampler(SubsetRandomSampler(range(self.num_buffer)), 128, False):\n",
        "                alpha, beta = self.acnet(s[index])[0]\n",
        "                distance = Beta(alpha, beta)\n",
        "                log_prob_a = distance.log_prob(a[index]).sum(1, True)\n",
        "                a_div_old_a = torch.exp(log_prob_a - log_prob_a_old[index])\n",
        "                obj1 = a_div_old_a * ad_v[index]\n",
        "                obj2 = torch.clamp(a_div_old_a, 1.0 - self.epsilon, 1.0 + self.epsilon) * ad_v[index]\n",
        "                action_loss = -torch.min(obj1, obj2).mean()\n",
        "                value_loss = F.smooth_l1_loss(self.acnet(s[index])[1], v_output[index])\n",
        "                loss = action_loss + 2 * value_loss\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "    def get_parameter_number(self):\n",
        "    \n",
        "        \ttotal_num = sum(p.numel() for p in self.acnet.parameters())\n",
        "        \ttrainable_num = sum(p.numel() for p in self.acnet.parameters() if p.requires_grad)\n",
        "        \tprint(self.acnet.state_dict)\n",
        "        \treturn total_num, trainable_num\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IJvK5CZjOU4",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLCBiNuKGhmH",
        "colab_type": "code",
        "outputId": "d1e4dad7-a3ae-4524-e00e-82c2be36b0ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def reward_memory():\n",
        "    count = 0\n",
        "    length = 100\n",
        "    history = np.zeros(length)\n",
        "\n",
        "    def memory(reward):\n",
        "        nonlocal count\n",
        "        history[count] = reward\n",
        "        count = (count + 1) % length\n",
        "        return np.mean(history)\n",
        "\n",
        "    return memory\n",
        "\n",
        "def reset(env):\n",
        "    av_r = reward_memory()\n",
        "    die = False\n",
        "    img_rgb = env.reset()\n",
        "    img_gray = np.dot(img_rgb[..., :], [0.299, 0.587, 0.114])\n",
        "    img_gray = img_gray / 128. - 1\n",
        "    stack = [img_gray] * 4\n",
        "    return stack, av_r, die\n",
        "\n",
        "\n",
        "def step(env, action, av_r, stack):\n",
        "    total_reward = 0\n",
        "    for i in range(8):\n",
        "        img_rgb, reward, die, _ = env.step(action)\n",
        "        if die:\n",
        "            reward += 100   # no penalty for die\n",
        "        if np.mean(img_rgb[:, :, 1]) > 185.0:   # green penalty\n",
        "            reward -= 0.05\n",
        "        total_reward += reward\n",
        "        done = True if av_r(reward) <= -0.1 else False\n",
        "        if done or die:\n",
        "            break\n",
        "    img_gray = np.dot(img_rgb[..., :], [0.299, 0.587, 0.114])\n",
        "    img_gray = img_gray / 128. - 1\n",
        "    stack.pop(0)\n",
        "    stack.append(img_gray)\n",
        "    return np.array(stack), total_reward, done, die\n",
        "\n",
        "agent = AgentTrain(0.5, 0.1, 10, 1000)\n",
        "\n",
        "# Print the model and parameters\n",
        "total_num, trainable_num = agent.get_parameter_number()\n",
        "print(\"Number of total parameters: \", total_num)\n",
        "print(\"Number of trainable parameters: \", trainable_num)\n",
        "\n",
        "env = gym.make(\"CarRacing-v0\")\n",
        "env.seed(0)\n",
        "reward_threshold = env.spec.reward_threshold\n",
        "stack, _, _ = reset(env)\n",
        "state = np.array(stack)\n",
        "training_records = []\n",
        "running_score = 0\n",
        "for i_ep in range(2500):\n",
        "    score = 0\n",
        "    stack, av_r, die = reset(env)\n",
        "    state = np.array(stack)\n",
        "    for t in range(1000):\n",
        "        action, log_prob_a = agent.policy(state)\n",
        "        state_, reward, done, die = step(env, action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]), av_r, stack)\n",
        "        if agent.save_state((state, action, log_prob_a, reward, state_)):\n",
        "            agent.update()\n",
        "        score += reward\n",
        "        state = state_\n",
        "        if done or die:\n",
        "            break\n",
        "    running_score = running_score * 0.99 + score * 0.01\n",
        "\n",
        "    if i_ep % 10 == 0:\n",
        "        print('i_ep: {}, score: {}'.format(i_ep, running_score))\n",
        "        torch.save(agent.acnet.state_dict(), os.path.join(checkpoint_path, 'ppo_net_params_{}.pth'.format(i_ep)))\n",
        "    if running_score > reward_threshold:\n",
        "        break\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.state_dict of ACNet(\n",
            "  (cnn_base): Sequential(\n",
            "    (0): Conv2d(4, 8, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (5): ReLU()\n",
            "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (7): ReLU()\n",
            "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (9): ReLU()\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (11): ReLU()\n",
            "  )\n",
            "  (v): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (alpha_head): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=3, bias=True)\n",
            "    (1): Softplus(beta=1, threshold=20)\n",
            "  )\n",
            "  (beta_head): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=3, bias=True)\n",
            "    (1): Softplus(beta=1, threshold=20)\n",
            "  )\n",
            ")>\n",
            "Number of total parameters:  445955\n",
            "Number of trainable parameters:  445955\n",
            "Track generation: 1143..1442 -> 299-tiles track\n",
            "Track generation: 1087..1369 -> 282-tiles track\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-524386a9977a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mav_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-524386a9977a>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(env, action, av_r, stack)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mimg_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdie\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m100\u001b[0m   \u001b[0;31m# no penalty for die\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"state_pixels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_buffer_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_color_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVP_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVP_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mget_image_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2153\u001b[0m         \u001b[0mglPixelStorei\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGL_PACK_ALIGNMENT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m         glReadPixels(x, y, self.width, self.height,\n\u001b[0;32m-> 2155\u001b[0;31m                      self.gl_format, GL_UNSIGNED_BYTE, buffer)\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0mglPopClientAttrib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_4F1gK-Tj8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}